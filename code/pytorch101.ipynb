{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67f14e0",
   "metadata": {},
   "source": [
    "# Import & Verify `PyTorch` is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8eca2cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bec2ec",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Let's start with something you should know, `numpy`. This is a scientific computing library which allows for each manipulation of vectors & matricies. `PyTorch` has a similar data structure called a `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39ff5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create some vectors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Create a matrix\n",
    "M = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "# Create a tensor with specific dimensions\n",
    "batch_size = 10\n",
    "dim = 3\n",
    "T = torch.randn(size=(batch_size, dim, dim))\n",
    "S = torch.rand(size=(batch_size, dim, dim))\n",
    "# `randn` generates a RANDom values drawn from a Normal distribution.\n",
    "print(T.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e452d",
   "metadata": {},
   "source": [
    "Q1) What is the default datatype for `Tensors` in `PyTorch`? Note it depends on how you initalize the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d543e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([ 4, 10, 18])\n",
      "tensor(32)\n",
      "tensor([14, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Addition / muliplication work element-wise, just like numpy\n",
    "print(a + b)\n",
    "print(a * b)\n",
    "\n",
    "# `@` operator does matrix multiplication\n",
    "print(a @ b)\n",
    "print(M @ a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c36f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40a26e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32)\n",
      "tensor([14, 32, 50])\n",
      "tensor(15)\n",
      "torch.Size([10, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# There's also einstein summation\n",
    "# Example: a @ b\n",
    "print(torch.einsum('i,i->', a, b))\n",
    "\n",
    "# Example: M @ a\n",
    "print(torch.einsum('ij,j->i', M, a))\n",
    "\n",
    "# Example: Tr(M)\n",
    "print(torch.einsum('ii->', M))\n",
    "\n",
    "# We can also do more complicated things, like batch matrix multiplication\n",
    "# So instead of running\n",
    "# > for i in range(batch_size):\n",
    "# >    C[i] = T[i] @ S[i]\n",
    "# We can do it all at once with einsum\n",
    "C = torch.einsum('bij,bjk->bik', T, S)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3eb205",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da34aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22a4c4e",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "In class, we discussed a simple multilayer perceptron. Recall it was mathematically defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{(\\ell)} & = W^{(\\ell)} \\sigma(z^{(\\ell-1)}) + b^{(\\ell)}, \\ \\text{s.t. }z^{(0)} = W^{(0)} x + b^{(0)}\\\\\n",
    "f_\\theta(x) & = z^{(L)}\n",
    "\\end{align}\n",
    "$$\n",
    "where the model parameter $\\theta = \\{W^{(\\ell)}, b^{(\\ell)}\\}_{\\ell=0}^L$ and $\\sigma(\\cdot)$ is a non-linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "356e01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513201aa",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c9fe9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25491d2",
   "metadata": {},
   "source": [
    "# Inference / Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43875fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
